{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDb 電影資料庫\n",
    "電影評論數據集   \n",
    "50000篇文字評論並且被標記為正或是負\n",
    "我們試著用將影評訓練集和標籤來建立模型並利用影評測試集計算正評和負評的正確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>election is a chinese mob movie or triads in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i was just watching a forensic files marathon ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>police story is a stunning series of set piece...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear readers the final battle between the rebe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i have seen the perfect son about three times ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a brilliant portrait of a traitor victor mclag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>if ever a potential movie must ve sounded like...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i d always wanted david duchovney to go into t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>perhaps if only to laugh at the way my favorit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>even though the story is light the movie flows...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  election is a chinese mob movie or triads in t...          1\n",
       "1  i was just watching a forensic files marathon ...          0\n",
       "2  police story is a stunning series of set piece...          1\n",
       "3  dear readers the final battle between the rebe...          1\n",
       "4  i have seen the perfect son about three times ...          1\n",
       "5  a brilliant portrait of a traitor victor mclag...          1\n",
       "6  if ever a potential movie must ve sounded like...          1\n",
       "7  i d always wanted david duchovney to go into t...          1\n",
       "8  perhaps if only to laugh at the way my favorit...          0\n",
       "9  even though the story is light the movie flows...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 到這邊算是整理好資料的格式了  之後從這邊讀資料就好了\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('./movie_data.csv', encoding='utf-8')\n",
    "\n",
    "df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 處理較大的數據 - out-of-core learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "上次那 50000 筆評論\n",
    "製作特徵向量以及分類還有調整參數 \n",
    "是相當耗時的\n",
    "\n",
    "為處理數據量過大而導致無法一次地儲存在Main Memory，\n",
    "Scikit-Learn能讓系統做一些調整，out-of-core learning便是其中一個策略。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-core (or “external memory”) learning \n",
    "適用在data過大,無法存放於主記憶體(RAM) 時\n",
    "\n",
    "以串流(stream)的方式從硬碟中讀取資料,批次處理\n",
    "\n",
    "但不是每種演算法都可以這麼做\n",
    "可以實作的項目如下: \n",
    "1.特徵提取 \n",
    "    sklearn.feature_extraction.text.HashingVectorizer for text documents.\n",
    "2.estimators:\n",
    "    Classification\n",
    "        sklearn.naive_bayes.MultinomialNB\n",
    "        sklearn.naive_bayes.BernoulliNB\n",
    "        sklearn.linear_model.Perceptron\n",
    "        sklearn.linear_model.SGDClassifier\n",
    "        sklearn.linear_model.PassiveAggressiveClassifier\n",
    "    Regression\n",
    "        sklearn.linear_model.SGDRegressor\n",
    "        sklearn.linear_model.PassiveAggressiveRegressor\n",
    "    Clustering\n",
    "        sklearn.cluster.MiniBatchKMeans\n",
    "    Decomposition / feature Extraction\n",
    "        sklearn.decomposition.MiniBatchDictionaryLearning\n",
    "        sklearn.decomposition.IncrementalPCA\n",
    "        sklearn.decomposition.LatentDirichletAllocation\n",
    "        sklearn.cluster.MiniBatchKMeans\n",
    "        \n",
    "而此時要使用 partial_fit 取代原先 fit 的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "# 每次讀取並回傳一個doc\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)  # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "#            print (text, label) \n",
    "            yield text, label\n",
    "            \n",
    "#yield 和 return 很像，只是當函數呼叫 return 時，\n",
    "#該函數 call stack (python 中是 frame) 就會被清除，\n",
    "#程式主導權回到呼叫該函數的手上。 \n",
    "#而 yield 會把程式主導權交給呼叫該函數的手上，但是他不會把函數的 \n",
    "#call stack 清除，因此下次呼叫時，可以從上次未執行的部分開始執行，\n",
    "#而不是重新建立一個新 stack。\n",
    "\n",
    "#內建函數 (function) next() ，回傳參數 (parameter) 迭代器中下一個數值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 備註:next() & yield-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list \n",
    "L = [x * x for x in range(10)]\n",
    "L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x000001BCBC52B3B8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把一個列表生成式的[]改成()，就創建了一個generator,從而節省大量的空間： \n",
    "g = (x * x for x in range(10))\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 要一個一個print出來，可以通過next()函數獲得generator的下一個返回值\n",
    "next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(g)\n",
    "# generator保存的是算法，每次調用next(g)，就計算出g的下一個元素的值，直到計算到最後一個元素，沒有更多的元素時，拋出StopIteration的錯誤。\n",
    "\n",
    "# 但正常是這樣用 \n",
    "#for n in g:\n",
    "#     print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fib函數實際上是定義了斐波拉契數列的推算規則\n",
    "def fib(max):\n",
    "    n, a, b = 0, 0, 1\n",
    "    while n < max:\n",
    "        print(b)\n",
    "        a, b = b, a + b\n",
    "        n = n + 1\n",
    "    return 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fib(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把fib函數變成generator，只需要把print(b)改為yield b就可以了：\n",
    "def fib(max):\n",
    "    n, a, b = 0, 0, 1\n",
    "    while n < max:\n",
    "        yield b\n",
    "        a, b = b, a + b\n",
    "        n = n + 1\n",
    "    return 'done'\n",
    "\n",
    "#函數是順序執行，遇到return語句或者最後一行函數語句就返回。\n",
    "#generator的函數，在每次調用next()的時候執行，遇到yield語句返回，\n",
    "#再次執行時從上次返回的yield語句處繼續執行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object fib at 0x000001BCBC52BD00>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = fib(6)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "done",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-468f0afdf1b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: done"
     ]
    }
   ],
   "source": [
    "next(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object stream_docs at 0x000002B996ABD678>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_docs(path='./movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('election is a chinese mob movie or triads in this case every two years an election is held to decide on a new leader and at first it seems a toss up between big d tony leung ka fai or as i know him the other tony leung and lok simon yam who was judge in full contact though once lok wins big d refuses to accept the choice and goes to whatever lengths he can to secure recognition as the new leader unlike any other asian film i watch featuring gangsters this one is not an action movie it has its bloody moments when necessary as in goodfellas but it s basically just a really effective drama there are a lot of characters which is really hard to keep track of but i think that plays into the craziness of it all a bit a 100 year old baton which is the symbol of power i mentioned before changes hands several times before things settle down and though it may appear that the film ends at the 65 or 70 minute mark there are still a couple big surprises waiting simon yam was my favorite character here and sort of anchors the picture election was quite the award winner at last year s hong kong film awards winning for best actor tony leung best picture best director johnny to who did heroic trio and best screenplay it also had nominations for cinematography editing film score which i loved and three more acting performances including yam ',\n",
       " 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='./movie_data.csv'))\n",
    "\n",
    "# 這樣就可以讀出第一份的文件  還有標籤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 一次會回傳　doc_stream　中的sizez份數\n",
    "# 輸入是串流 和份數\n",
    "\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CountVectorizer,TfidfVectorizer 需要將完整詞彙放在主記憶體中，無法做out-of-core learning\n",
    "#因此用　HashingVectorizer　會犧牲一點點準確度　\n",
    "\n",
    "#Linear classifiers (SVM, logistic regression, a.o.) with SGD training.\n",
    "#This estimator implements regularized linear models with stochastic gradient descent (SGD) learning\n",
    "\n",
    "# n_features=2**21,hash之後的特徵的數目\n",
    "# 選大一點是為了避免 hash碰撞, 但這樣會增加邏輯斯迴歸的係數\n",
    "\n",
    "# decode_error 如果有含不是給定編碼的字符的字節序列。 \n",
    "# 預設情況下，意味著將會引發UnicodeDecodeError。 讓我們忽略他:ignore 。 \n",
    "# tokenizer : 用前面定義的函數\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "doc_stream = stream_docs(path='./movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "備註 :  短網址在數學上 (或者密碼學上) 是一種雜湊函數的應用： 讓一個任意長度的字串轉換成固定長度、特定符號組成的字串。所以理論上， 一定會有機會發生碰撞，原因在於雜湊函數是把較大集合的內容， 對映到較小的集合當中。\n",
    "HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:53\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# 停用字刪除 and, a...\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# 開始進行 out-of-core learning \n",
    "#　45次　* 1000 筆資料\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.866\n"
     ]
    }
   ],
   "source": [
    "# 用剩下的 5000筆　　測試準確度\n",
    "\n",
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 用來測試的那 5000 筆資料當然不能浪費呀\n",
    "# 繼續拿來更新我們的模型\n",
    "\n",
    "clf = clf.partial_fit(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 序列化 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#一個程式被載入到記憶體當中運作，那麼在記憶體內的那個資料就被稱為程序(process)。\n",
    "#在程序運行的過程中，所有的變量都是在記憶體中，比如，定義一個dict：\n",
    "\n",
    "d = dict(name='Bob', age=20, score=88)\n",
    "\n",
    "#可以隨時修改變量，比如把name改成'Bill'\n",
    "#但是一旦程序結束，變量所佔用的記憶體就被操作系統全部回收。\n",
    "#如果沒有把修改後的'Bill'存儲到硬碟上，下次重新運行程序，變量又被初始化為'Bob'。\n",
    "\n",
    "#我們把變量從記憶體中變成可存儲或傳輸的過程稱之為序列化，\n",
    "#在Python中叫pickling，在其他語言中也被稱之為serialization，marshalling，flattening等等\n",
    "\n",
    "#序列化之後，就可以把序列化後的內容寫入硬碟，或者通過網絡傳輸到別的機器上。\n",
    "\n",
    "#反過來，把變量內容從序列化的對象重新讀到內存裡稱之為反序列化，即unpickling。\n",
    "\n",
    "#Python提供兩個模塊來實現序列化：cPickle和pickle。這兩個模塊功能是一樣的，\n",
    "#區別在於cPickle是C語言寫的，速度快，pickle是純Python寫的，速度慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x80\\x03}q\\x00(X\\x04\\x00\\x00\\x00nameq\\x01X\\x03\\x00\\x00\\x00Bobq\\x02X\\x03\\x00\\x00\\x00ageq\\x03K\\x14X\\x05\\x00\\x00\\x00scoreq\\x04KXu.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "d = dict(name='Bob', age=20, score=88)\n",
    "pickle.dumps(d)\n",
    "\n",
    "#pickle.dumps()方法把任意對象序列化成一個str，然後，就可以把這個str寫入文件。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('dump.txt', 'wb')\n",
    "pickle.dump(d, f)\n",
    "f.close()\n",
    "\n",
    "#寫入的dump.txt文件的內容，這些是Python保存的對象內部信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 20, 'name': 'Bob', 'score': 88}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#把內容讀到一個str，然後用pickle.loads()方法反序列化出對象，\n",
    "#也可以直接用pickle.load()方法從一個file-like Object中直接反序列化出對象。\n",
    "f = open('dump.txt', 'rb')\n",
    "d = pickle.load(f)\n",
    "f.close()\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上是序列化簡介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 序列化 scikit-learn estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "# 建立一個　movieclassifier　目錄，　pkl_objects　子目錄\n",
    "\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "#  將停用字　以及　分類器　序列化　存在硬碟中 ; protocol=4 : python3.4 中的　pickle協定\n",
    "\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
    "pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting movieclassifier/vectorizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile movieclassifier/vectorizer.py\n",
    "# 將向量化的腳本也寫入硬碟\n",
    "# 要使用的時候　需要載入當下的工作環境之中\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(\n",
    "                os.path.join(cur_dir, \n",
    "                'pkl_objects', \n",
    "                'stopwords.pkl'), 'rb'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 測試一下能不能正常載入\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from vectorizer import vect\n",
    "\n",
    "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Probability: 82.18%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "label = {0:'negative', 1:'positive'}\n",
    "# 測試一下一個簡單的句子\n",
    "example = ['I love this movie']\n",
    "# 句子轉成向量　回傳類別標籤以及相對應的機率\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[clf.predict(X)[0]], clf.predict_proba(X).max()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用　SQLite 　存預測結果的回饋意見"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLite 　是開放原始碼的資料庫引擎\n",
    "運作時不需要單獨的伺服器\n",
    "適合簡單的ｗｅｂ系統\n",
    "可以理解為　單獨的　資料庫檔案\n",
    "我們的系統可以直接存取　\n",
    "在幾乎所有的ＯＳ都可以執行\n",
    "\n",
    "最重要的是！　　python 的標準函式庫　\n",
    "包含了 SQLite API : sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "# 從新建立兩份範例資料　　\n",
    "if os.path.exists('reviews.sqlite'):\n",
    "    os.remove('reviews.sqlite')\n",
    "\n",
    "conn = sqlite3.connect('reviews.sqlite')\n",
    "c = conn.cursor()\n",
    "c.execute('CREATE TABLE review_db (review TEXT, sentiment INTEGER, date TEXT)')\n",
    "\n",
    "example1 = 'I love this movie'\n",
    "c.execute(\"INSERT INTO review_db (review, sentiment, date) VALUES (?, ?, DATETIME('now'))\", (example1, 1))\n",
    "\n",
    "example2 = 'I disliked this movie'\n",
    "c.execute(\"INSERT INTO review_db (review, sentiment, date) VALUES (?, ?, DATETIME('now'))\", (example2, 0))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 檢查是否寫入範例資料\n",
    "conn = sqlite3.connect('reviews.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"SELECT * FROM review_db WHERE date BETWEEN '2017-01-01 10:10:10' AND DATETIME('now')\")\n",
    "results = c.fetchall()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I love this movie', 1, '2017-07-19 15:42:37'), ('I disliked this movie', 0, '2017-07-19 15:42:37')]\n"
     ]
    }
   ],
   "source": [
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
